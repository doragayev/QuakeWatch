apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: quakewatch-alerts
  namespace: monitoring
  labels:
    app: quakewatch
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  - name: quakewatch.rules
    rules:
    # Application Health Alerts
    - alert: QuakeWatchDown
      expr: up{job="quakewatch"} == 0
      for: 1m
      labels:
        severity: critical
        service: quakewatch
      annotations:
        summary: "QuakeWatch application is down"
        description: "QuakeWatch application has been down for more than 1 minute"
        runbook_url: "https://docs.example.com/runbooks/quakewatch-down"

    - alert: QuakeWatchHighErrorRate
      expr: rate(quakewatch_errors_total[5m]) > 0.1
      for: 2m
      labels:
        severity: warning
        service: quakewatch
      annotations:
        summary: "QuakeWatch high error rate detected"
        description: "QuakeWatch error rate is {{ $value }} errors/sec for more than 2 minutes"
        runbook_url: "https://docs.example.com/runbooks/high-error-rate"

    - alert: QuakeWatchHighResponseTime
      expr: histogram_quantile(0.95, rate(quakewatch_request_duration_seconds_bucket[5m])) > 5
      for: 3m
      labels:
        severity: warning
        service: quakewatch
      annotations:
        summary: "QuakeWatch high response time"
        description: "QuakeWatch 95th percentile response time is {{ $value }}s for more than 3 minutes"
        runbook_url: "https://docs.example.com/runbooks/high-response-time"

    - alert: QuakeWatchNoRequests
      expr: rate(quakewatch_requests_total[10m]) == 0
      for: 5m
      labels:
        severity: warning
        service: quakewatch
      annotations:
        summary: "QuakeWatch receiving no requests"
        description: "QuakeWatch has received no requests for more than 10 minutes"
        runbook_url: "https://docs.example.com/runbooks/no-requests"

    - alert: QuakeWatchHighMemoryUsage
      expr: (process_resident_memory_bytes{job="quakewatch"} / 1024 / 1024) > 500
      for: 5m
      labels:
        severity: warning
        service: quakewatch
      annotations:
        summary: "QuakeWatch high memory usage"
        description: "QuakeWatch memory usage is {{ $value }}MB for more than 5 minutes"
        runbook_url: "https://docs.example.com/runbooks/high-memory-usage"

    - alert: QuakeWatchHighCPUUsage
      expr: rate(process_cpu_seconds_total{job="quakewatch"}[5m]) > 0.8
      for: 5m
      labels:
        severity: warning
        service: quakewatch
      annotations:
        summary: "QuakeWatch high CPU usage"
        description: "QuakeWatch CPU usage is {{ $value }} for more than 5 minutes"
        runbook_url: "https://docs.example.com/runbooks/high-cpu-usage"

    # Earthquake Processing Alerts
    - alert: QuakeWatchEarthquakeProcessingStopped
      expr: rate(quakewatch_earthquakes_total[10m]) == 0
      for: 10m
      labels:
        severity: warning
        service: quakewatch
      annotations:
        summary: "QuakeWatch earthquake processing stopped"
        description: "No earthquakes have been processed for more than 10 minutes"
        runbook_url: "https://docs.example.com/runbooks/earthquake-processing-stopped"

    - alert: QuakeWatchHighEarthquakeVolume
      expr: rate(quakewatch_earthquakes_total[5m]) > 10
      for: 2m
      labels:
        severity: info
        service: quakewatch
      annotations:
        summary: "QuakeWatch high earthquake volume"
        description: "High volume of earthquakes being processed: {{ $value }} earthquakes/sec"
        runbook_url: "https://docs.example.com/runbooks/high-earthquake-volume"

    # API and Data Processing Alerts
    - alert: QuakeWatchAPICallFailure
      expr: rate(quakewatch_api_calls_total{api_name!=""}[5m]) == 0
      for: 5m
      labels:
        severity: warning
        service: quakewatch
      annotations:
        summary: "QuakeWatch API calls stopped"
        description: "No API calls have been made for more than 5 minutes"
        runbook_url: "https://docs.example.com/runbooks/api-calls-stopped"

    - alert: QuakeWatchDataProcessingStopped
      expr: rate(quakewatch_data_points_processed_total[10m]) == 0
      for: 10m
      labels:
        severity: warning
        service: quakewatch
      annotations:
        summary: "QuakeWatch data processing stopped"
        description: "No data points have been processed for more than 10 minutes"
        runbook_url: "https://docs.example.com/runbooks/data-processing-stopped"

  - name: kubernetes.rules
    rules:
    # Kubernetes Cluster Alerts
    - alert: KubernetesPodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: critical
        service: kubernetes
      annotations:
        summary: "Pod {{ $labels.pod }} is crash looping"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is crash looping"
        runbook_url: "https://docs.example.com/runbooks/pod-crash-looping"

    - alert: KubernetesPodNotReady
      expr: kube_pod_status_phase{phase!="Running",phase!="Succeeded"} > 0
      for: 5m
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: "Pod {{ $labels.pod }} is not ready"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is in {{ $labels.phase }} phase"
        runbook_url: "https://docs.example.com/runbooks/pod-not-ready"

    - alert: KubernetesHighCPUUsage
      expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: "High CPU usage on node {{ $labels.instance }}"
        description: "CPU usage is {{ $value }}% on node {{ $labels.instance }}"
        runbook_url: "https://docs.example.com/runbooks/high-cpu-usage"

    - alert: KubernetesHighMemoryUsage
      expr: 100 - ((node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100) > 90
      for: 5m
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: "High memory usage on node {{ $labels.instance }}"
        description: "Memory usage is {{ $value }}% on node {{ $labels.instance }}"
        runbook_url: "https://docs.example.com/runbooks/high-memory-usage"

    - alert: KubernetesDiskSpaceLow
      expr: 100 - ((node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100) > 90
      for: 5m
      labels:
        severity: critical
        service: kubernetes
      annotations:
        summary: "Low disk space on node {{ $labels.instance }}"
        description: "Disk usage is {{ $value }}% on node {{ $labels.instance }}"
        runbook_url: "https://docs.example.com/runbooks/low-disk-space"

    - alert: KubernetesNodeDown
      expr: up{job="node-exporter"} == 0
      for: 1m
      labels:
        severity: critical
        service: kubernetes
      annotations:
        summary: "Node {{ $labels.instance }} is down"
        description: "Node {{ $labels.instance }} has been down for more than 1 minute"
        runbook_url: "https://docs.example.com/runbooks/node-down"

    - alert: KubernetesDeploymentReplicasMismatch
      expr: kube_deployment_status_replicas_available != kube_deployment_spec_replicas
      for: 5m
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: "Deployment {{ $labels.deployment }} replicas mismatch"
        description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} has {{ $value }} available replicas but {{ $labels.spec_replicas }} desired"
        runbook_url: "https://docs.example.com/runbooks/deployment-replicas-mismatch"

  - name: monitoring.rules
    rules:
    # Monitoring System Alerts
    - alert: PrometheusDown
      expr: up{job="prometheus"} == 0
      for: 1m
      labels:
        severity: critical
        service: prometheus
      annotations:
        summary: "Prometheus is down"
        description: "Prometheus has been down for more than 1 minute"
        runbook_url: "https://docs.example.com/runbooks/prometheus-down"

    - alert: GrafanaDown
      expr: up{job="grafana"} == 0
      for: 1m
      labels:
        severity: critical
        service: grafana
      annotations:
        summary: "Grafana is down"
        description: "Grafana has been down for more than 1 minute"
        runbook_url: "https://docs.example.com/runbooks/grafana-down"

    - alert: AlertManagerDown
      expr: up{job="alertmanager"} == 0
      for: 1m
      labels:
        severity: critical
        service: alertmanager
      annotations:
        summary: "AlertManager is down"
        description: "AlertManager has been down for more than 1 minute"
        runbook_url: "https://docs.example.com/runbooks/alertmanager-down"

    - alert: PrometheusTargetDown
      expr: up == 0
      for: 1m
      labels:
        severity: warning
        service: prometheus
      annotations:
        summary: "Prometheus target {{ $labels.instance }} is down"
        description: "Prometheus target {{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute"
        runbook_url: "https://docs.example.com/runbooks/prometheus-target-down"
